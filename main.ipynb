{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import math\n",
    "from fileinput import filename\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Welcome to Huy Ngo Feature Selection Algorithm.\\n')\n",
    "    \n",
    "    #totalFeatures = int(input('Please enter total number of features'))\n",
    "    #data_file = input ('Please enter the data file name\\n').strip() #removing the whitespace of the file\n",
    "    #while not os.path.isfile(data_file):\n",
    "        #data_file = input('Please enter a valid file name')\n",
    "    \n",
    "    #classifier = Classifier(data_file)\n",
    "    #classifier.train()\n",
    "    #with open(data_file, 'w+') as file: #read/write position at start\n",
    "        #file.truncate(0) \n",
    "        #https://mkyong.com/python/python-difference-between-r-w-and-a-in-open/\n",
    "        \n",
    "    #https://www.geeksforgeeks.org/python-set-method/\n",
    "    filename = input ('Please enter the data file name\\n').strip() #removing the whitespace of the file\n",
    "    \n",
    "    while not os.path.isfile(filename):\n",
    "        filename = input('Please enter a valid file name')\n",
    "    \n",
    "    print('Training data...\\n')\n",
    "    \n",
    "    helper = Helper(filename)\n",
    "    helper.start()\n",
    "    \n",
    "    print('Done!\\n')\n",
    " \n",
    "    algorithm = int(input('Type the number of the algorithm you want to run.\\n'\n",
    "                            '1. Forward Selection\\n'\n",
    "                            '2. Backward Elimination\\n'\n",
    "                            '3. Huy\\'s Special Algorithm\\n'))\n",
    "    if algorithm == 1:\n",
    "        helper.forwardSelection()\n",
    "    if algorithm == 2:\n",
    "        helper.backwardElimination()\n",
    "    else:\n",
    "        #did not implement a special algorithm\n",
    "        return\n",
    " #Both forward-selection and backward-elimination are greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class Classifier:\n",
    "#All you need to do is to keep the training instances in memory and when a new data point is given for classification, \n",
    "#compute its distance to the training points and return the class label of the nearest training point. \n",
    "#You may want to use specialized data structures, sorting, indexing, etc. to speed things up, but this is not mandatory.\n",
    "    def __init__(self,filename):\n",
    "        #self.object_to_classify = 0\n",
    "        #self.label_object_to_classify = 0\n",
    "        self.dataset = filename\n",
    "        self.kRows = 0 #num of k rows\n",
    "        self.contents = [] #will read and store the data set\n",
    "        self.number_correctly_classfied = 0 #number of correct classification\n",
    "        self.accuracy = 0 #accuracy = correctClass/kRows\n",
    "        self.numFeatures = 0 #get number of features from the data set, excluding class label column\n",
    "        self.feature1Class1 = []\n",
    "        self.feature2Class1 = []\n",
    "        self.feature1Class2 = []\n",
    "        self.feature2Class2 = []\n",
    "        self.featuresNorm_arr = None\n",
    "        self.features = []\n",
    "        #self.defaultRate = None\n",
    "        #with open(self.dataset, 'w+') as file: #read/write position at start\n",
    "            #self.dataset.truncate(0) \n",
    "        #https://mkyong.com/python/python-difference-between-r-w-and-a-in-open/\n",
    "        #self.featuresColumn = None\n",
    "        #self.scaled_df = None\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        #The input to the Train method is the set of training instances (or their IDs), \n",
    "        #no output for this method. Note that the class label for each instance is provided along with the feature vector.\n",
    "    \n",
    "        #with open(self.dataset, 'r') as data_set:\n",
    "            #https://docs.python.org/3/library/csv.html\n",
    "            #https://stackoverflow.com/questions/19143667/how-to-read-a-csv-without-the-first-column\n",
    "            #https://stackoverflow.com/questions/24955475/reading-in-a-text-file-as-a-numpy-array-with-float-values\n",
    "            #n_cols = len(data_set.readline().split(' '))\n",
    "            with open(self.dataset, 'r') as f:\n",
    "                self.contents = list(csv.reader(f, delimiter=' ', skipinitialspace=True)) #converts the csv object into a list\n",
    "                self.kRows = len(self.contents) #returns the number of rows or k objects in csv file\n",
    "                self.numFeatures = len(self.contents[0]) - 1 #need to offset by 1 because the first column is class label\n",
    "                \n",
    "            #Normalized data\n",
    "#             for row in range(len(self.contents)):\n",
    "#                 currRow = self.contents[row]\n",
    "#                 self.features.append(float(currRow))\n",
    "                    \n",
    "#             imp_mean = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0) \n",
    "#             imputer = imp_mean.fit([self.features])\n",
    "#             self.features = imputer.transform([self.features])\n",
    "            \n",
    "#             features_Norm = np.array([self.features])\n",
    "                \n",
    "#             n_data = features_Norm.shape\n",
    "#             features_train_datasetNorm = features_Norm.reshape((n_data))\n",
    "#             self.featuresNorm_arr = preprocessing.normalize([features_train_datasetNorm])\n",
    "\n",
    "\n",
    "            #self.numFeatures = np.loadtxt(self.dataset, delimiter=' ', usecols=range(1,n_cols+1))\n",
    "\n",
    "                                    \n",
    "            \n",
    "    #Test: The input to the Test method is a test instance (or its ID) and the output is the predicted class label. \n",
    "    \n",
    "    \n",
    "    def tracing(self,statement):\n",
    "         with open(self.dataset.replace('.txt', '') + '_NN_backward_tracing.txt', 'a') as f:\n",
    "                    f.write(str(statement) + '\\n') \n",
    "                    \n",
    "\n",
    "        \n",
    "    def test(self, data): #leave_one_out_cross_validation time\n",
    "        #nearest_neighbor_label = 0\n",
    "        for i in range(len(data)):\n",
    "            #object_to_classify = int(float(self.contents[1][i]))\n",
    "            label_object_to_classify = int(float(self.contents[i][0]))\n",
    "            #https://www.geeksforgeeks.org/python-infinity/     \n",
    "            #initial distance for NN is infinity\n",
    "            nearest_neighbor_distance = float('inf')\n",
    "            nearest_neighbor_location = float('inf')\n",
    "            for k in range(len(data)):                \n",
    "                #print('Ask if ', str(i), ' is nearest neighbor with ', str(k), '\\\\n')\n",
    "                #self.tracing(f'Ask if {str(i)} is nearest neighbor with  {str(k)}')\n",
    "                if k != i: #no comparing to self\n",
    "                    #print('Ask if ', str(i), ' is nearest neighbor with ', str(k))\n",
    "                    #d = distance.euclidean(data[int(i)], data[int(k)]),\n",
    "                    d = np.sqrt(np.sum(np.square(np.array(data[i], dtype=float) - np.array(data[k], dtype=float))))\n",
    "                    #print('Distance between ', data[i], ' and ', data[k], ' is ', d)\n",
    "                    #self.tracing(f'Distance between  {data[i]}  and  {data[k]}  is  {d}')\n",
    "                    if d < nearest_neighbor_distance:\n",
    "                        nearest_neighbor_distance = d\n",
    "                        nearest_neighbor_location = k\n",
    "                        nearest_neighbor_label = int(float(self.contents[k][0]))\n",
    "            if label_object_to_classify == nearest_neighbor_label:\n",
    "                self.number_correctly_classfied += 1\n",
    "        self.accuracy = float(self.number_correctly_classfied) / float(self.kRows)\n",
    "        #self.tracing(f'Accuracy is:  {self.accuracy}')\n",
    "        #self.number_correctly_classfied = 0 #reset\n",
    "        \n",
    "    def plotFeatures_Normalized(self, feature1 : int, feature2 : int):\n",
    "        #https://www.w3schools.com/python/matplotlib_markers.asp\n",
    "        \n",
    "        for row in range(len(self.contents)):\n",
    "            currRow = self.contents[row]\n",
    "            if round(float(currRow[0]), 0) == 1:\n",
    "                self.feature1Class1.append(round(float(currRow[feature1]), 2))\n",
    "                self.feature2Class1.append(round(float(currRow[feature2]), 2))\n",
    "            else:\n",
    "                self.feature1Class2.append(round(float(currRow[feature1]), 2))\n",
    "                self.feature2Class2.append(round(float(currRow[feature2]), 2))\n",
    "                \n",
    "                \n",
    "        #Normalize Data\n",
    "        #https://stackoverflow.com/questions/34972142/sklearn-logistic-regression-valueerror-found-array-with-dim-3-estimator-expec\n",
    "        #https://www.journaldev.com/45109/normalize-data-in-python\n",
    "        \n",
    "        feature1Class1Norm = np.array([self.feature1Class1])\n",
    "        feature2Class1Norm = np.array([self.feature2Class1])\n",
    "        feature1Class2Norm = np.array([self.feature1Class2])\n",
    "        feature2Class2Norm = np.array([self.feature2Class2])\n",
    "        \n",
    "        \n",
    "        feature1Class1Norm_x, feature1Class1Norm_y = feature1Class1Norm.shape\n",
    "        feature2Class1Norm_x, feature2Class1Norm_y = feature2Class1Norm.shape\n",
    "        feature1Class2Norm_x, feature1Class2Norm_y = feature1Class2Norm.shape\n",
    "        feature2Class2Norm_x, feature2Class2Norm_y = feature2Class2Norm.shape\n",
    "\n",
    "        \n",
    "        class1_f1_train_datasetNorm = feature1Class1Norm.reshape((feature1Class1Norm_x * feature1Class1Norm_y))\n",
    "        class1_f2_train_datasetNorm = feature2Class1Norm.reshape((feature1Class1Norm_x * feature1Class1Norm_y))\n",
    "        class2_f1_train_datasetNorm = feature1Class2Norm.reshape((feature1Class2Norm_x * feature1Class2Norm_y))\n",
    "        class2_f2_train_datasetNorm = feature2Class2Norm.reshape((feature2Class2Norm_x * feature2Class2Norm_y))\n",
    "        \n",
    "        \n",
    "        feature1Class1Norm_arr = preprocessing.normalize([class1_f1_train_datasetNorm])\n",
    "        feature2Class1Norm_arr = preprocessing.normalize([class1_f2_train_datasetNorm])\n",
    "        feature1Class2Norm_arr = preprocessing.normalize([class2_f1_train_datasetNorm])\n",
    "        feature2Class2Norm_arr = preprocessing.normalize([class2_f2_train_datasetNorm])\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        fig1.tight_layout()\n",
    "        \n",
    "        ax.set_ylabel('Feature ' + str(feature2))\n",
    "        ax.set_xlabel('Feature ' + str(feature1))\n",
    "        ax.set_title(self.dataset.replace('.txt',''))\n",
    "        \n",
    "        ax1.set_ylabel('Feature ' + str(feature2))\n",
    "        ax1.set_xlabel('Feature ' + str(feature1))\n",
    "        ax1.set_title('Normalized')\n",
    "\n",
    "        ax.plot(self.feature1Class1, self.feature2Class1, 'o', color='red', markerfacecolor='none', label='Class 1')\n",
    "        ax.plot(self.feature1Class2, self.feature2Class2, 'o', color='green', markerfacecolor='none', label='Class 2')\n",
    "        \n",
    "        #Normalized plot\n",
    "        ax1.plot(feature1Class1Norm_arr, feature2Class1Norm_arr, 'o', color='red', markerfacecolor='none')\n",
    "        ax1.plot(feature1Class2Norm_arr, feature2Class2Norm_arr, 'o', color='green', markerfacecolor='none')\n",
    "        \n",
    "        leg = ax.legend()\n",
    "        #leg1 = ax1.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def getFile(self):\n",
    "        return self.dataset\n",
    "    \n",
    "    def reset(self):\n",
    "        self.number_correctly_classfied = 0\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def accuracyOutput(self): #returns accuracy for that data\n",
    "        return self.accuracy\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator:\n",
    "    #Why we need it: We are going to use the evaluation module (validator) later in Part III when we want to evaluate different feature subsets \n",
    "    #(nodes in the search tree) by calculating accuracies. Remember that our greedy algorithm from part I did not use a real evaluation \n",
    "    #function; instead the dummy evaluation function assigned a random value (accuracy) to each feature subset (node).\n",
    "    def __init__(self, currSet, classifier : Classifier):\n",
    "        self.classifier = classifier\n",
    "        self.currSet = currSet\n",
    "        self.accuracy = None\n",
    "        self.data = []\n",
    "    \n",
    "    def validator(self): #create the validator data to pass into classifier test function\n",
    "        #might need a deepcopy here\n",
    "        validatedList = []\n",
    "        # print(self.subset.subset)\n",
    "        for row in range(len(self.classifier.contents)):\n",
    "            tempRow = [] \n",
    "            for feature in self.currSet:\n",
    "                tempRow.append(self.classifier.contents[row][feature])\n",
    "            validatedList.append(tempRow)\n",
    "        self.classifier.test(validatedList)\n",
    "        self.accuracy = self.classifier.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.classifier = Classifier(self.filename)\n",
    "        self.validation = None\n",
    "        self.accuracy = 0\n",
    "        self.curr = None\n",
    "        self.featureSet = set()#Initialize an empty set\n",
    "        self.bestSubsets = {} #empty initially\n",
    "        self.finalAccuracy = 0\n",
    "        self.defaultRate = None\n",
    "        self.normalizeAccuracy = None\n",
    "        \n",
    "    def start(self):\n",
    "        print('Please wait while I normalize the data...\\n')\n",
    "        self.classifier.train()\n",
    "        print('Done!')\n",
    "        \n",
    "        featureChoice1 = input('Please choose the 1st feature to plot (then press enter): \\n').split('\\n')\n",
    "        featureChoice2 = input('Please choose 2nd feature to plot (then press enter): \\n').split('\\n')\n",
    "        self.classifier.plotFeatures_Normalized(int(featureChoice1[0]), int(featureChoice2[0]))\n",
    "        \n",
    "        if os.path.isfile(self.filename.replace('.txt', '') + '_trace.txt'):\n",
    "            with open(self.filename.replace('.txt', '') + '_trace.txt', 'r+') as f:\n",
    "                    f.truncate(0) \n",
    "    \n",
    "    def numFeatures(self):\n",
    "        return self.classifier.numFeatures\n",
    "    \n",
    "    def kRows(self):\n",
    "        return self.classifier.kRows\n",
    "        \n",
    "    def getFile(self):\n",
    "        return self.classifier.dataset\n",
    "\n",
    "    def tracing(self, statement):\n",
    "         with open(self.getFile().replace('.txt', '') + '_backward_selection_tracing.txt', 'a') as f:\n",
    "                    f.write(str(statement) + '\\n')  \n",
    "\n",
    "    def displayBestInfo(self):\n",
    "        #self.curr = curr\n",
    "        #self.accuracy = round(accuracy * 100, 2)\n",
    "        print(f'Feature set {self.featureSet} was best, accuracy is {round(self.finalAccuracy * 100, 2)}%\\n')\n",
    "        #self.tracing(f'Feature set {self.featureSet} was best, accuracy is {round(self.finalAccuracy * 100, 2)}%\\n')\n",
    "    \n",
    "    def displayInfo(self):\n",
    "        #https://www.geeksforgeeks.org/round-function-python/\n",
    "        #self.curr = curr\n",
    "        #self.accuracy = round(accuracy * 100, 2)\n",
    "        print(f'Using feature(s) {self.curr} accuracy is {round(self.accuracy * 100, 2)}%')\n",
    "        #self.tracing(f'Using feature(s) {self.curr} accuracy is {round(self.accuracy * 100, 2)}%')\n",
    "        \n",
    "    def defaultAccuracy(self):\n",
    "        defaultAccuracy_dict = {}\n",
    "        for row in self.classifier.contents:\n",
    "            if row[0] not in defaultAccuracy_dict:\n",
    "                defaultAccuracy_dict[row[0]] = 1\n",
    "            else:\n",
    "                defaultAccuracy_dict[row[0]] += 1\n",
    "        self.defaultRate = round(float(max(defaultAccuracy_dict.values()))/float(self.kRows()) * 100, 2)\n",
    "        return self.defaultRate\n",
    "    \n",
    "    # def normalizeAccuracy(self):\n",
    "    #     normalizeAccuracy_dict = {}\n",
    "    #     for row in self.classifier.featuresNorm_arr:\n",
    "    #         if row[0] not in normalizeAccuracy_dict:\n",
    "    #             normalizeAccuracy_dict[row[0]] = 1\n",
    "    #         else:\n",
    "    #             normalizeAccuracy_dict[row[0]] += 1\n",
    "    #     self.normalizeAccuracy = round(float(max(normalizeAccuracy_dict.values()))/float(self.kRows()) * 100, 2)\n",
    "    #     return self.normalizeAccuracy\n",
    "        \n",
    "    #https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/\n",
    "#function accuracy = leave_one_out_cross_validation(data,current_set,feature_to_add)\n",
    "#    accuracy = rand;        % This is a testing stub only\n",
    "#end\n",
    "#def stubEval(): \n",
    "    #https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html\n",
    "    #perform eval function on node, and then return a random %\n",
    "    #return (np.random.random() * 100) #returns a random float \n",
    "    \n",
    "    def realEval(self, currSet):\n",
    "        self.curr = currSet\n",
    "        self.validation = Validator(self.curr, self.classifier)\n",
    "        self.validation.validator()#calling test()\n",
    "        self.accuracy = self.classifier.accuracyOutput() #return accuracy\n",
    "        self.classifier.reset()\n",
    "        return self.accuracy\n",
    "\n",
    "#function feature_search_demo(data)\n",
    "#current_set_of_features = []; % Initialize an empty set\n",
    "    #for i = 1 : size(data,2)-1 \n",
    "        #disp(['On the ',num2str(i),'th level of the search tree'])\n",
    "        #feature_to_add_at_this_level = [];\n",
    "        #best_so_far_accuracy = 0;    \n",
    "        #for k = 1 : size(data,2)-1 \n",
    "            #if isempty(intersect(current_set_of_features,k)) % Only consider adding, if not already added.\n",
    "                #disp(['--Considering adding the ', num2str(k),' feature'])\n",
    "                #accuracy = leave_one_out_cross_validation(data,current_set_of_features,k+1);\n",
    "            \n",
    "                #if accuracy > best_so_far_accuracy\n",
    "                    #best_so_far_accuracy = accuracy;\n",
    "                    #feature_to_add_at_this_level = k;            \n",
    "                \n",
    "                #end\n",
    "            #end\n",
    "        #end\n",
    "        #current_set_of_features(i) =  feature_to_add_at_this_level;\n",
    "        #disp(['On level ', num2str(i),' i added feature ', num2str(feature_to_add_at_this_level), ' to current set'])\n",
    "    #end \n",
    "#end\n",
    "\n",
    "    def forwardSelection(self): #it starts with the full set of features and removes one feature at a time\n",
    "        #https://www.geeksforgeeks.org/python-set-method/\n",
    "        #filename = input ('Please enter the data file name\\n').strip() #removing the whitespace of the file\n",
    "\n",
    "        #while not os.path.isfile(filename):\n",
    "            #filename = input('Please enter a valid file name')\n",
    "\n",
    "        #print('Training data...\\n')\n",
    "\n",
    "        #helper = Helper(filename)\n",
    "        #helper.start()\n",
    "\n",
    "        #print('Done!\\n')\n",
    "        print(f'Running nearest neighbor with no features (default rate), using leaving out validation, I get an accuracy of {self.defaultAccuracy()}%\\n')\n",
    "        #print(f'Running nearest neighbor with no features (normalize rate), using leaving out validation, I get an accuracy of {self.normalizeAccuracy()}%\\n')\n",
    "        #self.tracing(f'Running nearest neighbor with no features (default rate), using leaving out validation, I get an accuracy of {self.defaultAccuracy()}%')\n",
    "\n",
    "        #helper = Helper(classifier)\n",
    "        print(f'The dataset has {self.numFeatures()} features (not including the class attribute), with {self.kRows()} instances\\n')\n",
    "        #self.tracing(f'The dataset has {self.numFeatures()} features (not including the class attribute), with {self.kRows()} instances')\n",
    "        #featureSet = set()#Initialize an empty set\n",
    "        #bestSubsets = {} #empty initially\n",
    "        #moved featureSet and bestSubsets to Helper class instead local\n",
    "        #features = totalFeatures\n",
    "        #finalAccuracy = 0 move finalAccuracy to Helper class instead local\n",
    "        accuracy = 0\n",
    "\n",
    "        #totalFeatures = helper.classifier.numFeatures\n",
    "        for i in range(1, self.numFeatures()):#where n is the total number of features\n",
    "            currBestAccuracy = -1\n",
    "            #featureSet = set()#Initialize an empty set\n",
    "            #featureToAdd = set()\n",
    "            #bestAccuracy = {}\n",
    "            final_k = 0\n",
    "            print('On the ', str(i), 'th  level of the search tree\\n')\n",
    "            #self.tracing(f'On the {str(i)} th  level of the search tree\\n')\n",
    "            for k in range(1, self.numFeatures() + 1):#for loop from index 1 to numbers of features present\n",
    "                if k not in self.featureSet:\n",
    "                    print('--Considering adding the', str(k), 'feature')\n",
    "                    #self.tracing(f'--Considering adding the {str(k)} feature')\n",
    "                    # Deep copy since Python does pass by reference for function calls\n",
    "                    curr = copy.deepcopy(self.featureSet)\n",
    "                    curr.add(k)\n",
    "                    accuracy = self.realEval(curr)\n",
    "                    #bestAccuracy = accuracy\n",
    "                    self.displayInfo()\n",
    "                    #helper.tracing(helper.displayInfo())\n",
    "                    k_temp = k\n",
    "                    if (accuracy > currBestAccuracy):#current accuracy is strictly greater than the best accuracy so far\n",
    "                        currBestAccuracy = accuracy\n",
    "                        #featureToAdd.add(k_temp) #add k_temp to featureToAdd\n",
    "                        self.finalAccuracy = accuracy \n",
    "                        updated_Curr = curr\n",
    "                        final_k = k_temp\n",
    "            print()\n",
    "            #self.displayBestInfo()\n",
    "            #helper.tracing(helper.displayBestInfo())\n",
    "            self.featureSet.add(final_k) #current_set_of_features(i) =  feature_to_add_at_this_level;\n",
    "            #updated_Curr = copy.deepcopy(self.featureSet)\n",
    "            #self.bestSubsets[self.finalAccuracy] = updated_Curr\n",
    "            self.displayBestInfo()\n",
    "            #self.featureSet.add(final_k) #current_set_of_features(i) =  feature_to_add_at_this_level;\n",
    "            print('On level ', str(i), ' I added feature ', str(self.featureSet),' to current set\\n')\n",
    "            #self.tracing(f'On level {str(i)} I added feature {str(self.featureSet)} to current set\\n')\n",
    "            accuracyReduced = False\n",
    "            #https://www.w3schools.com/python/ref_dictionary_keys.asp\n",
    "            for j in self.bestSubsets.keys(): #check to see if accuracy did decrease\n",
    "                if (self.finalAccuracy < j):\n",
    "                    accuracyReduced = True\n",
    "                    break\n",
    "            if (accuracyReduced):\n",
    "                print('\"(Warning, Accuracy has decreased!)\"')\n",
    "                #self.tracing('\"(Warning, Accuracy has decreased!)\"')\n",
    "                break\n",
    "\n",
    "            self.bestSubsets[self.finalAccuracy] = updated_Curr #initialize bestSubset accuracy to store with the best subset for the current iteration\n",
    "\n",
    "        print(f'Finished search!! The best feature subset is {str(self.bestSubsets[max(self.bestSubsets.keys())])}, which is an accuracy of {round(max(self.bestSubsets.keys()) * 100,2)}%')\n",
    "        #self.tracing(f'Finished search!! The best feature subset is {str(self.bestSubsets[max(self.bestSubsets.keys())])}, which is an accuracy of {round(max(self.bestSubsets.keys()) * 100,2)}%')\n",
    "    \n",
    "\n",
    "    def backwardElimination(self):#starts with the full set of features and removes one feature at a time.\n",
    "        #https://www.geeksforgeeks.org/python-set-method/\n",
    "        #filename = input ('Please enter the data file name\\n').strip() #removing the whitespace of the file\n",
    "\n",
    "        #while not os.path.isfile(filename):\n",
    "            #filename = input('Please enter a valid file name')\n",
    "\n",
    "        #helper = Helper(filename)\n",
    "        #helper.start()\n",
    "\n",
    "        #helper = Helper(classifier)\n",
    "\n",
    "        #featureSet = set()#Initialize an empty set\n",
    "        #bestSubsets = {} #empty initially\n",
    "        #finalAccuracy = 0\n",
    "        #features = totalFeatures\n",
    "        print(f'Running nearest neighbor with no features (default rate), using leaving out validation, I get an accuracy of {self.defaultAccuracy()}%\\n')\n",
    "        #print(f'Running nearest neighbor with no features (normalize rate), using leaving out validation, I get an accuracy of {self.normalizeAccuracy()}%\\n')\n",
    "        #self.tracing(f'Running nearest neighbor with no features (default rate), using leaving out validation, I get an accuracy of {self.defaultAccuracy()}%')\n",
    "        accuracy = 0\n",
    "\n",
    "        #totalFeatures = helper.classifier.numFeatures\n",
    "        print(f'The dataset has {self.numFeatures()} features (not including the class attribute), with {self.kRows()} instances\\n')\n",
    "        #self.tracing(f'The dataset has {self.numFeatures()} features (not including the class attribute), with {self.kRows()} instances')\n",
    "        for features in range(1, self.numFeatures() + 1):\n",
    "            self.featureSet.add(features)\n",
    "\n",
    "        curr = copy.deepcopy(self.featureSet)\n",
    "        accuracy = self.realEval(curr)\n",
    "\n",
    "        self.displayInfo()\n",
    "        #helper.tracing(helper.displayInfo())\n",
    "        #self.displayBestInfo()\n",
    "        print(f'Feature set {self.featureSet} was best, accuracy is {round(self.accuracy * 100, 2)}%\\n')\n",
    "        #helper.tracing(helper.displayBestInfo())\n",
    "        print('On level 1 I added feature ', str(self.featureSet),' to current set\\n')\n",
    "        #self.tracing(f'On level 1 I added feature {str(self.featureSet)} to current set\\n')\n",
    "\n",
    "        self.bestSubsets[accuracy] = curr\n",
    "        for i in range(1, self.numFeatures() - 1):#where n is the total number of features\n",
    "            currBestAccuracy = -1\n",
    "            #bestAccuracy = {}\n",
    "            final_k = 0\n",
    "            print('On the ', str(i+1), 'th  level of the search tree\\n')\n",
    "            #self.tracing(f'On level 1 I added feature {str(self.featureSet)} to current set\\n')\n",
    "            for k in range(1, self.numFeatures() + 1):#for loop from index 1 to numbers of features present\n",
    "                if k in self.featureSet:\n",
    "                    print('--Considering removing the', str(k), 'feature')\n",
    "                    #self.tracing(f'--Considering removing the {str(k)} feature')\n",
    "                    # Deep copy since Python does pass by reference for function calls\n",
    "                    curr = copy.deepcopy(self.featureSet)\n",
    "                    curr.remove(k)\n",
    "                    accuracy = self.realEval(curr)\n",
    "                    #bestAccuracy = accuracy\n",
    "                    self.displayInfo()\n",
    "                    #helper.tracing(helper.displayInfo())\n",
    "                    k_temp = k\n",
    "                    if (accuracy > currBestAccuracy):#current accuracy is strictly greater than the best accuracy so far\n",
    "                        currBestAccuracy = accuracy\n",
    "                        self.finalAccuracy = accuracy \n",
    "                        final_k = k_temp\n",
    "            print()\n",
    "            #self.displayBestInfo()\n",
    "            self.featureSet.remove(final_k) #current_set_of_features(i) =  feature_to_add_at_this_level;\n",
    "            updated_Curr = copy.deepcopy(self.featureSet)\n",
    "            self.bestSubsets[self.finalAccuracy] = updated_Curr\n",
    "            self.displayBestInfo()\n",
    "            #self.displayBestInfo()\n",
    "            #helper.tracing(helper.displayBestInfo())\n",
    "            print('On level ', str(i+1), ' I added feature ', str(self.featureSet),' to current set\\n')\n",
    "            #self.tracing(f'On level {str(i+1)} I added feature {str(self.featureSet)} to current set\\n')\n",
    "            #featureSet.remove(final_k) #current_set_of_features(i) =  feature_to_add_at_this_level;\n",
    "            accuracyReduced = False\n",
    "            #https://www.w3schools.com/python/ref_dictionary_keys.asp\n",
    "            for j in self.bestSubsets.keys(): #check to see if accuracy did decrease\n",
    "                if (self.finalAccuracy < j):\n",
    "                    accuracyReduced = True\n",
    "                    break\n",
    "            if (accuracyReduced):\n",
    "                print('\"(Warning, Accuracy has decreased!)\"')\n",
    "                #self.tracing('\"(Warning, Accuracy has decreased!)\"')\n",
    "                break\n",
    "\n",
    "            self.bestSubsets[self.finalAccuracy] = updated_Curr #initialize bestSubset accuracy to store with the best subset for the current iteration\n",
    "\n",
    "        print(f'Finished search!! The best feature subset is {str(self.bestSubsets[max(self.bestSubsets.keys())])}, which is an accuracy of {round(max(self.bestSubsets.keys()) * 100,2)}%')\n",
    "        #self.tracing(f'Finished search!! The best feature subset is {str(self.bestSubsets[max(self.bestSubsets.keys())])}, which is an accuracy of {round(max(self.bestSubsets.keys()) * 100,2)}%')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
